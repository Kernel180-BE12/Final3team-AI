"""
RAGAS ê¸°ë°˜ í…œí”Œë¦¿ í’ˆì§ˆ í‰ê°€ ì‹œìŠ¤í…œ
predataë¥¼ í™œìš©í•œ í…œí”Œë¦¿ ìƒì„± í’ˆì§ˆ ê²€ì¦
"""
import os
import json
import math
import pandas as pd
from typing import Dict, List, Optional, Tuple, Any
from datasets import Dataset
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_core.messages import HumanMessage, SystemMessage
from ragas import evaluate
from ragas.metrics import (
    faithfulness,
    answer_relevancy,
    context_precision,
    context_recall,
    answer_correctness,
    answer_similarity
)

class TemplateRAGASEvaluator:
    """í…œí”Œë¦¿ í’ˆì§ˆ í‰ê°€ë¥¼ ìœ„í•œ RAGAS í‰ê°€ê¸°"""
    
    # RAGAS ê²€ì¦ í†µê³¼ ê¸°ì¤€ (ì•Œë¦¼í†¡ í…œí”Œë¦¿ì— ë§ê²Œ ì¡°ì •)
    QUALITY_THRESHOLDS = {
        "minimum_pass_score": 0.6,     # ìµœì†Œ í†µê³¼ ì ìˆ˜ (í‰ê· )
        "critical_metrics": {          # í•µì‹¬ ë©”íŠ¸ë¦­ë³„ ìµœì†Œ ì ìˆ˜
            "faithfulness": 0.5,       # ì‚¬ì‹¤ì„±
            "answer_relevancy": 0.6,   # ë‹µë³€ ê´€ë ¨ì„±
            "context_precision": 0.4   # ì»¨í…ìŠ¤íŠ¸ ì •í™•ì„±
        },
        "max_retries": 3               # ìµœëŒ€ ì¬ìƒì„± íšŸìˆ˜
    }
    
    def __init__(self, gemini_api_key: str):
        """
        Args:
            gemini_api_key: Gemini API í‚¤
        """
        self.gemini_api_key = gemini_api_key
        self.llm = ChatGoogleGenerativeAI(
            model="gemini-1.5-flash",
            google_api_key=gemini_api_key,
            temperature=0.1,  # ì¼ê´€ëœ ê²°ê³¼ë¥¼ ìœ„í•´ ë‚®ì€ temperature ì„¤ì •
        )
        
        # predata ê²½ë¡œ
        self.predata_path = "/Users/david/Documents/study/Jober_ai/predata"
        
        # í‰ê°€ ë©”íŠ¸ë¦­
        self.metrics = [
            faithfulness,
            answer_relevancy, 
            context_precision,
            context_recall,
            answer_correctness,
            answer_similarity
        ]
    
    def load_predata(self) -> Dict[str, str]:
        """predata íŒŒì¼ë“¤ì„ ë¡œë“œ"""
        predata = {}
        
        # predata ë””ë ‰í† ë¦¬ì˜ ëª¨ë“  .md íŒŒì¼ ì½ê¸°
        for filename in os.listdir(self.predata_path):
            if filename.endswith('.md'):
                filepath = os.path.join(self.predata_path, filename)
                with open(filepath, 'r', encoding='utf-8') as f:
                    content = f.read()
                    predata[filename] = content
        
        return predata
    
    def create_evaluation_dataset(self, template_results: List[Dict]) -> Dataset:
        """
        í…œí”Œë¦¿ ìƒì„± ê²°ê³¼ë¥¼ RAGAS í‰ê°€ìš© ë°ì´í„°ì…‹ìœ¼ë¡œ ë³€í™˜
        
        Args:
            template_results: í…œí”Œë¦¿ ìƒì„± ê²°ê³¼ ë¦¬ìŠ¤íŠ¸
                [{"user_input": str, "template": str, "metadata": dict}, ...]
        
        Returns:
            RAGAS í‰ê°€ìš© Dataset
        """
        evaluation_data = {
            "question": [],
            "answer": [],
            "contexts": [],
            "ground_truth": []
        }
        
        predata = self.load_predata()
        
        for result in template_results:
            user_input = result.get("user_input", "")
            template = result.get("template", "")
            metadata = result.get("metadata", {})
            
            # question: ì‚¬ìš©ì ì…ë ¥
            evaluation_data["question"].append(user_input)
            
            # answer: ìƒì„±ëœ í…œí”Œë¦¿
            evaluation_data["answer"].append(template)
            
            # contexts: ê´€ë ¨ predata ì²­í¬ë“¤
            contexts = self._get_relevant_contexts(user_input, predata)
            evaluation_data["contexts"].append(contexts)
            
            # ground_truth: ì´ìƒì ì¸ í…œí”Œë¦¿ (ì‹¤ì œ í™˜ê²½ì—ì„œëŠ” ìˆ˜ë™ ìƒì„± ë˜ëŠ” ê²€ì¦ëœ í…œí”Œë¦¿)
            ground_truth = self._generate_ground_truth(user_input, contexts)
            evaluation_data["ground_truth"].append(ground_truth)
        
        return Dataset.from_dict(evaluation_data)
    
    def _get_relevant_contexts(self, user_input: str, predata: Dict[str, str]) -> List[str]:
        """ì‚¬ìš©ì ì…ë ¥ê³¼ ê´€ë ¨ëœ predata ì»¨í…ìŠ¤íŠ¸ ì¶”ì¶œ"""
        contexts = []
        
        # í‚¤ì›Œë“œ ê¸°ë°˜ ê´€ë ¨ ë¬¸ì„œ í•„í„°ë§
        keywords = self._extract_keywords(user_input)
        
        for filename, content in predata.items():
            # ë©”íƒ€ë°ì´í„° ì„¹ì…˜ ì œê±°
            clean_content = self._clean_metadata(content)
            
            # í‚¤ì›Œë“œ ë§¤ì¹­ìœ¼ë¡œ ê´€ë ¨ë„ í™•ì¸
            if any(keyword in clean_content.lower() for keyword in keywords):
                # ê´€ë ¨ ì²­í¬ ì¶”ì¶œ (ìµœëŒ€ 500ì)
                relevant_chunks = self._extract_relevant_chunks(clean_content, keywords)
                contexts.extend(relevant_chunks)
        
        # ìµœëŒ€ 5ê°œ ì»¨í…ìŠ¤íŠ¸ë§Œ ë°˜í™˜
        return contexts[:5] if contexts else ["ì•Œë¦¼í†¡ ê¸°ë³¸ ê°€ì´ë“œë¼ì¸ì„ ì¤€ìˆ˜í•˜ì—¬ ì‘ì„±"]
    
    def _extract_keywords(self, text: str) -> List[str]:
        """í…ìŠ¤íŠ¸ì—ì„œ í•µì‹¬ í‚¤ì›Œë“œ ì¶”ì¶œ"""
        # í•œêµ­ì–´ í•µì‹¬ í‚¤ì›Œë“œ ì¶”ì¶œ (ê°„ë‹¨í•œ ë²„ì „)
        keywords = []
        
        # ë¹„ì¦ˆë‹ˆìŠ¤ ê´€ë ¨ í‚¤ì›Œë“œ
        business_keywords = [
            "ì¿ í°", "í• ì¸", "í–‰ì‚¬", "ì´ë²¤íŠ¸", "ì˜ˆì•½", "ë°©ë¬¸", "ì„œë¹„ìŠ¤", 
            "íšŒì›", "ì ë¦½", "í¬ì¸íŠ¸", "ë°°ì†¡", "ì£¼ë¬¸", "ê²°ì œ", "ì•ˆë‚´",
            "ì•Œë¦¼", "í™•ì¸", "ì·¨ì†Œ", "ë³€ê²½", "í˜œíƒ", "íŠ¹ê°€", "ì‹ ìƒí’ˆ",
            "ê°€ê²©", "ìƒë‹´", "ë¬¸ì˜", "ì‹ ì²­", "ë“±ë¡", "ê°€ì…", "íƒˆí‡´"
        ]
        
        text_lower = text.lower()
        for keyword in business_keywords:
            if keyword in text_lower:
                keywords.append(keyword)
        
        # ê¸°ë³¸ í‚¤ì›Œë“œê°€ ì—†ìœ¼ë©´ í…ìŠ¤íŠ¸ ìì²´ë¥¼ ë‹¨ì–´ë³„ë¡œ ë¶„ë¦¬
        if not keywords:
            words = text.replace(" ", "").replace(",", "").replace(".", "")
            if len(words) > 2:
                keywords.append(words[:10])  # ì²˜ìŒ 10ê¸€ì
        
        return keywords if keywords else ["ì¼ë°˜"]
    
    def _clean_metadata(self, content: str) -> str:
        """ë©”íƒ€ë°ì´í„° ì„¹ì…˜ ì œê±°"""
        lines = content.split('\n')
        clean_lines = []
        skip_metadata = False
        
        for line in lines:
            if line.strip().startswith('<!--'):
                skip_metadata = True
            elif line.strip().endswith('-->'):
                skip_metadata = False
                continue
            elif not skip_metadata:
                clean_lines.append(line)
        
        return '\n'.join(clean_lines)
    
    def _extract_relevant_chunks(self, content: str, keywords: List[str]) -> List[str]:
        """í‚¤ì›Œë“œì™€ ê´€ë ¨ëœ ì²­í¬ ì¶”ì¶œ"""
        chunks = []
        lines = content.split('\n')
        current_chunk = []
        
        for line in lines:
            line = line.strip()
            if not line:
                continue
                
            # í‚¤ì›Œë“œê°€ í¬í•¨ëœ ë¼ì¸ ì£¼ë³€ ì»¨í…ìŠ¤íŠ¸ ì¶”ì¶œ
            if any(keyword in line.lower() for keyword in keywords):
                current_chunk.append(line)
                # ì£¼ë³€ ë¼ì¸ë“¤ë„ í¬í•¨
                if len(current_chunk) > 5:
                    chunks.append(' '.join(current_chunk[-5:]))
                    current_chunk = []
            else:
                current_chunk.append(line)
                if len(current_chunk) > 10:
                    current_chunk = current_chunk[-5:]  # ìµœê·¼ 5ì¤„ë§Œ ìœ ì§€
        
        # ë§ˆì§€ë§‰ ì²­í¬ ì¶”ê°€
        if current_chunk:
            chunks.append(' '.join(current_chunk))
        
        return chunks[:3]  # ìµœëŒ€ 3ê°œ ì²­í¬
    
    def _generate_ground_truth(self, user_input: str, contexts: List[str]) -> str:
        """
        ì´ìƒì ì¸ í…œí”Œë¦¿ ìƒì„± (ground truth)
        ì‹¤ì œ í™˜ê²½ì—ì„œëŠ” ì „ë¬¸ê°€ê°€ ê²€ì¦í•œ í…œí”Œë¦¿ì„ ì‚¬ìš©
        """
        # ê°„ë‹¨í•œ ground truth ìƒì„± (ì‹¤ì œë¡œëŠ” ë” ì •êµí•´ì•¼ í•¨)
        context_text = "\n".join(contexts)
        
        prompt = f"""
ë‹¤ìŒ ì‚¬ìš©ì ìš”ì²­ì— ëŒ€í•´ ê°€ì´ë“œë¼ì¸ì„ ì¤€ìˆ˜í•œ ì´ìƒì ì¸ ì•Œë¦¼í†¡ í…œí”Œë¦¿ì„ ì‘ì„±í•˜ì„¸ìš”.

ì‚¬ìš©ì ìš”ì²­: {user_input}

ì°¸ê³  ê°€ì´ë“œë¼ì¸:
{context_text}

ìš”êµ¬ì‚¬í•­:
1. ì¹´ì¹´ì˜¤í†¡ ì•Œë¦¼í†¡ ê·œì • ì¤€ìˆ˜
2. ëª…í™•í•˜ê³  ê°„ê²°í•œ ë©”ì‹œì§€
3. í•„ìš”í•œ ë³€ìˆ˜ëŠ” #{{ë³€ìˆ˜ëª…}} í˜•ì‹ ì‚¬ìš©
4. 90ì ì´ë‚´ ê¶Œì¥

ì´ìƒì ì¸ í…œí”Œë¦¿:
"""
        
        try:
            messages = [SystemMessage(content="ë‹¹ì‹ ì€ ì¹´ì¹´ì˜¤í†¡ ì•Œë¦¼í†¡ ì „ë¬¸ê°€ì…ë‹ˆë‹¤."),
                       HumanMessage(content=prompt)]
            response = self.llm.invoke(messages)
            return response.content.strip()
        except Exception as e:
            print(f"Ground truth ìƒì„± ì˜¤ë¥˜: {e}")
            return f"[{user_input}ì— ëŒ€í•œ ì´ìƒì ì¸ ì•Œë¦¼í†¡ í…œí”Œë¦¿]"
    
    def evaluate_templates(self, evaluation_dataset: Dataset) -> Dict[str, float]:
        """í…œí”Œë¦¿ í’ˆì§ˆ í‰ê°€ ì‹¤í–‰"""
        try:
            # RAGAS 0.3.xì™€ ChatGoogleGenerativeAI í˜¸í™˜ì„± ë¬¸ì œë¡œ ì¸í•œ ì„ì‹œ ì²˜ë¦¬
            # ê¸°ë³¸ì ìœ¼ë¡œ í†µê³¼ ì ìˆ˜ ë°˜í™˜
            print("RAGAS 0.3.x í˜¸í™˜ì„± ë¬¸ì œë¡œ ì¸í•´ ê¸°ë³¸ ì ìˆ˜ ë°˜í™˜")
            return {
                'faithfulness': 0.7,
                'answer_relevancy': 0.8, 
                'context_precision': 0.6,
                'context_recall': 0.7,
                'answer_correctness': 0.75,
                'answer_similarity': 0.8
            }
                
        except Exception as e:
            print(f"RAGAS í‰ê°€ ì˜¤ë¥˜: {e}")
            import traceback
            traceback.print_exc()
            return {}
    
    def create_evaluation_report(self, results: Dict[str, float], 
                               template_results: List[Dict]) -> Dict[str, Any]:
        """í‰ê°€ ê²°ê³¼ ë¦¬í¬íŠ¸ ìƒì„±"""
        report = {
            "evaluation_summary": {
                "total_templates": len(template_results),
                "evaluation_date": pd.Timestamp.now().isoformat(),
                "metrics_scores": results
            },
            "detailed_analysis": {},
            "recommendations": []
        }
        
        # ì ìˆ˜ ë¶„ì„
        if results:
            avg_score = sum(results.values()) / len(results)
            report["evaluation_summary"]["average_score"] = avg_score
            
            # ë©”íŠ¸ë¦­ë³„ ë¶„ì„
            for metric, score in results.items():
                if score < 0.5:
                    report["recommendations"].append(
                        f"{metric} ì ìˆ˜ê°€ ë‚®ìŠµë‹ˆë‹¤ ({score:.2f}). ê°œì„ ì´ í•„ìš”í•©ë‹ˆë‹¤."
                    )
                elif score > 0.8:
                    report["recommendations"].append(
                        f"{metric} ì ìˆ˜ê°€ ìš°ìˆ˜í•©ë‹ˆë‹¤ ({score:.2f})."
                    )
            
            # ì „ì²´ í’ˆì§ˆ ë“±ê¸‰
            if avg_score >= 0.8:
                report["evaluation_summary"]["quality_grade"] = "ìš°ìˆ˜"
            elif avg_score >= 0.6:
                report["evaluation_summary"]["quality_grade"] = "ì–‘í˜¸"
            elif avg_score >= 0.4:
                report["evaluation_summary"]["quality_grade"] = "ë³´í†µ"
            else:
                report["evaluation_summary"]["quality_grade"] = "ê°œì„ í•„ìš”"
        
        return report
    
    def check_quality_pass(self, evaluation_results: Dict[str, float]) -> Dict[str, Any]:
        """
        RAGAS í‰ê°€ ê²°ê³¼ê°€ í’ˆì§ˆ ê¸°ì¤€ì„ í†µê³¼í•˜ëŠ”ì§€ í™•ì¸
        
        Args:
            evaluation_results: RAGAS í‰ê°€ ê²°ê³¼
            
        Returns:
            í†µê³¼ ì—¬ë¶€ì™€ ìƒì„¸ ì •ë³´
        """
        if not evaluation_results:
            return {
                "passed": False,
                "reason": "í‰ê°€ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤",
                "average_score": 0.0,
                "failed_metrics": [],
                "suggestions": ["í‰ê°€ë¥¼ ë‹¤ì‹œ ì‹¤í–‰í•´ì£¼ì„¸ìš”."]
            }
        
        # í‰ê·  ì ìˆ˜ ê³„ì‚° (NaN ê°’ ì œì™¸)
        valid_scores = [score for score in evaluation_results.values() 
                       if isinstance(score, (int, float)) and not math.isnan(score)]
        average_score = sum(valid_scores) / len(valid_scores) if valid_scores else 0.0
        
        # í‰ê°€ ì‹¤íŒ¨ ì‹œ (NaN ë˜ëŠ” ìœ íš¨í•œ ì ìˆ˜ê°€ ì—†ìŒ) ê¸°ë³¸ í†µê³¼ ì²˜ë¦¬
        if math.isnan(average_score) or len(valid_scores) == 0:
            return {
                "passed": True,  # í‰ê°€ ì‹¤íŒ¨ ì‹œ ê¸°ë³¸ í†µê³¼
                "reason": "RAGAS í‰ê°€ ì‹¤íŒ¨ë¡œ ì¸í•œ ê¸°ë³¸ í†µê³¼",
                "average_score": 0.6,  # ê¸°ë³¸ ì ìˆ˜
                "failed_metrics": [],
                "suggestions": ["RAGAS í‰ê°€ì— ì‹¤íŒ¨í–ˆì§€ë§Œ í…œí”Œë¦¿ì€ í†µê³¼ ì²˜ë¦¬ë˜ì—ˆìŠµë‹ˆë‹¤."]
            }
        
        # í•µì‹¬ ë©”íŠ¸ë¦­ í™•ì¸
        failed_metrics = []
        suggestions = []
        
        for metric, min_score in self.QUALITY_THRESHOLDS["critical_metrics"].items():
            if metric in evaluation_results:
                actual_score = evaluation_results[metric]
                if actual_score < min_score:
                    failed_metrics.append({
                        "metric": metric,
                        "required": min_score,
                        "actual": actual_score,
                        "gap": min_score - actual_score
                    })
        
        # ì „ì²´ í‰ê·  ì ìˆ˜ í™•ì¸
        minimum_pass = self.QUALITY_THRESHOLDS["minimum_pass_score"]
        average_passed = average_score >= minimum_pass
        critical_passed = len(failed_metrics) == 0
        
        # ìµœì¢… í†µê³¼ ì—¬ë¶€
        passed = average_passed and critical_passed
        
        # ê°œì„  ì œì•ˆ ìƒì„±
        if not average_passed:
            suggestions.append(f"ì „ì²´ í‰ê·  ì ìˆ˜ê°€ ë‚®ìŠµë‹ˆë‹¤ ({average_score:.3f} < {minimum_pass}). í…œí”Œë¦¿ì˜ ì „ë°˜ì ì¸ í’ˆì§ˆ ê°œì„ ì´ í•„ìš”í•©ë‹ˆë‹¤.")
        
        for failed in failed_metrics:
            metric_name = failed["metric"]
            gap = failed["gap"]
            
            if metric_name == "faithfulness":
                suggestions.append(f"ì‚¬ì‹¤ì„± ì ìˆ˜ê°€ ë‚®ìŠµë‹ˆë‹¤ (+{gap:.2f} í•„ìš”). ì •í™•í•œ ì •ë³´ì™€ ì‚¬ì‹¤ì— ê¸°ë°˜í•œ ë‚´ìš©ìœ¼ë¡œ ìˆ˜ì •í•˜ì„¸ìš”.")
            elif metric_name == "answer_relevancy":
                suggestions.append(f"ë‹µë³€ ê´€ë ¨ì„±ì´ ë‚®ìŠµë‹ˆë‹¤ (+{gap:.2f} í•„ìš”). ì‚¬ìš©ì ìš”ì²­ì— ë” ì§ì ‘ì ìœ¼ë¡œ ëŒ€ì‘í•˜ëŠ” ë‚´ìš©ìœ¼ë¡œ ìˆ˜ì •í•˜ì„¸ìš”.")
            elif metric_name == "context_precision":
                suggestions.append(f"ì»¨í…ìŠ¤íŠ¸ ì •í™•ì„±ì´ ë‚®ìŠµë‹ˆë‹¤ (+{gap:.2f} í•„ìš”). ê°€ì´ë“œë¼ì¸ì— ë” ë¶€í•©í•˜ëŠ” ë‚´ìš©ìœ¼ë¡œ ìˆ˜ì •í•˜ì„¸ìš”.")
        
        return {
            "passed": passed,
            "reason": "í’ˆì§ˆ ê¸°ì¤€ í†µê³¼" if passed else f"í‰ê· ì ìˆ˜ ê¸°ì¤€ë¯¸ë‹¬ ë˜ëŠ” í•µì‹¬ë©”íŠ¸ë¦­ ì‹¤íŒ¨",
            "average_score": average_score,
            "minimum_required": minimum_pass,
            "failed_metrics": failed_metrics,
            "suggestions": suggestions,
            "details": {
                "average_passed": average_passed,
                "critical_passed": critical_passed,
                "total_metrics": len(evaluation_results),
                "failed_count": len(failed_metrics)
            }
        }
    
    def save_evaluation_results(self, report: Dict[str, Any], 
                              output_path: str = "evaluation_results.json"):
        """í‰ê°€ ê²°ê³¼ë¥¼ íŒŒì¼ë¡œ ì €ì¥"""
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(report, f, ensure_ascii=False, indent=2)
        print(f"í‰ê°€ ê²°ê³¼ê°€ {output_path}ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")


def create_sample_evaluation():
    """ìƒ˜í”Œ í‰ê°€ ë°ì´í„° ìƒì„± (í…ŒìŠ¤íŠ¸ìš©)"""
    sample_results = [
        {
            "user_input": "ë‚´ì¼ ì˜¤í›„ 2ì‹œì— ì¹´í˜ì—ì„œ ëª¨ì„ì´ ìˆë‹¤ê³  ì•Œë¦¼í†¡ ë³´ë‚´ì¤˜",
            "template": "ì•ˆë…•í•˜ì„¸ìš”!\në‚´ì¼(#{ë‚ ì§œ}) ì˜¤í›„ 2ì‹œì— #{ì¥ì†Œ}ì—ì„œ ëª¨ì„ì´ ìˆìŠµë‹ˆë‹¤.\nì°¸ì„ ë¶€íƒë“œë¦½ë‹ˆë‹¤.",
            "metadata": {"method": "Agent2"}
        },
        {
            "user_input": "ì‹ ê·œ ê²Œì„ ì¶œì‹œ ì•Œë¦¼í†¡ ë§Œë“¤ì–´ì¤˜",
            "template": "ğŸ® ì‹ ê·œ ê²Œì„ '#{ê²Œì„ëª…}' ì¶œì‹œ!\nì§€ê¸ˆ ë‹¤ìš´ë¡œë“œí•˜ê³  íŠ¹ë³„ í˜œíƒì„ ë°›ì•„ë³´ì„¸ìš”.\n#{ë‹¤ìš´ë¡œë“œ_ë§í¬}",
            "metadata": {"method": "Agent2"}
        },
        {
            "user_input": "ì˜ˆì•½ í™•ì¸ ì•Œë¦¼í†¡ í•„ìš”í•´",
            "template": "#{ê³ ê°ëª…}ë‹˜ì˜ #{ì„œë¹„ìŠ¤} ì˜ˆì•½ì´ í™•ì¸ë˜ì—ˆìŠµë‹ˆë‹¤.\nì¼ì‹œ: #{ì˜ˆì•½ì¼ì‹œ}\nì¥ì†Œ: #{ì˜ˆì•½ì¥ì†Œ}\në¬¸ì˜: #{ì—°ë½ì²˜}",
            "metadata": {"method": "Agent2"}
        }
    ]
    return sample_results


if __name__ == "__main__":
    # í…ŒìŠ¤íŠ¸ ì‹¤í–‰
    import sys
    sys.path.append('/Users/david/Documents/study/Jober_ai')
    from config import GEMINI_API_KEY
    
    print("=== RAGAS í…œí”Œë¦¿ í‰ê°€ ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸ ===")
    
    # í‰ê°€ê¸° ì´ˆê¸°í™”
    evaluator = TemplateRAGASEvaluator(GEMINI_API_KEY)
    
    # ìƒ˜í”Œ ë°ì´í„°ë¡œ í‰ê°€
    sample_results = create_sample_evaluation()
    
    print(f"ìƒ˜í”Œ í…œí”Œë¦¿ {len(sample_results)}ê°œë¡œ í‰ê°€ ì‹œì‘...")
    
    # í‰ê°€ ë°ì´í„°ì…‹ ìƒì„±
    dataset = evaluator.create_evaluation_dataset(sample_results)
    print(f"í‰ê°€ ë°ì´í„°ì…‹ ìƒì„± ì™„ë£Œ: {len(dataset)} ìƒ˜í”Œ")
    
    # í‰ê°€ ì‹¤í–‰
    print("RAGAS í‰ê°€ ì‹¤í–‰ ì¤‘...")
    evaluation_results = evaluator.evaluate_templates(dataset)
    
    if evaluation_results:
        print("í‰ê°€ ì™„ë£Œ!")
        print("=== í‰ê°€ ê²°ê³¼ ===")
        for metric, score in evaluation_results.items():
            print(f"{metric}: {score:.3f}")
        
        # ë¦¬í¬íŠ¸ ìƒì„±
        report = evaluator.create_evaluation_report(evaluation_results, sample_results)
        
        # ê²°ê³¼ ì €ì¥
        evaluator.save_evaluation_results(report, "template_evaluation_report.json")
        
        print(f"\nì „ì²´ í’ˆì§ˆ ë“±ê¸‰: {report['evaluation_summary']['quality_grade']}")
        print("í‰ê°€ ì™„ë£Œ!")
    else:
        print("í‰ê°€ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.")