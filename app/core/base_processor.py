import re
import json
import numpy as np
from typing import List, Dict, Tuple, Optional
from pathlib import Path
from langchain_google_genai import ChatGoogleGenerativeAI, GoogleGenerativeAIEmbeddings
from langchain_openai import ChatOpenAI
from config.llm_providers import get_llm_manager, LLMProvider
from .index_manager import get_index_manager

class BaseTemplateProcessor:
    """í…œí”Œë¦¿ ì²˜ë¦¬ ê¸°ë³¸ í´ë˜ìŠ¤"""

    def __init__(self, api_key: str, gemini_model: str = "gemini-2.0-flash-exp"):
        self.api_key = api_key

        # LLM ê´€ë¦¬ìë¥¼ í†µí•´ LLM ì„ íƒ
        llm_manager = get_llm_manager()
        primary_config = llm_manager.get_primary_config()
        fallback_config = llm_manager.get_fallback_config()

        try:
            if primary_config and primary_config.provider == LLMProvider.OPENAI:
                print(f"âœ… BaseProcessor: OpenAI {primary_config.model_name} ì‚¬ìš© ì¤‘")
                self.gemini_model = ChatOpenAI(
                    model=primary_config.model_name,
                    api_key=primary_config.api_key,
                    temperature=primary_config.temperature,
                    max_tokens=primary_config.max_tokens
                )
                self.provider = "openai"
                # ì„ë² ë”©ì€ ì—¬ì „íˆ Gemini ì‚¬ìš© (OpenAI ì„ë² ë”©ì€ ë³„ë„ ì²˜ë¦¬ í•„ìš”)
                if fallback_config and fallback_config.provider == LLMProvider.GEMINI:
                    self.embeddings_model = GoogleGenerativeAIEmbeddings(
                        model="models/embedding-001",
                        google_api_key=fallback_config.api_key
                    )
                    print("ğŸ“ ì„ë² ë”©: Gemini ì‚¬ìš©")
                else:
                    print("âš ï¸ ì„ë² ë”©: í´ë°± ì‚¬ìš©")
                    self.embeddings_model = None
            elif primary_config and primary_config.provider == LLMProvider.GEMINI:
                print(f"âœ… BaseProcessor: Gemini {primary_config.model_name} ì‚¬ìš© ì¤‘")
                self.gemini_model = ChatGoogleGenerativeAI(
                    model=primary_config.model_name,
                    google_api_key=primary_config.api_key,
                    temperature=primary_config.temperature
                )
                self.embeddings_model = GoogleGenerativeAIEmbeddings(
                    model="models/embedding-001",
                    google_api_key=primary_config.api_key
                )
                self.provider = "gemini"
                print("ğŸ“ ì„ë² ë”©: Gemini ì‚¬ìš©")
            else:
                # í´ë°±
                print("âš ï¸ BaseProcessor: ê¸°ë³¸ ì„¤ì •ìœ¼ë¡œ í´ë°±")
                self.gemini_model = ChatGoogleGenerativeAI(model=gemini_model, google_api_key=api_key)
                self.embeddings_model = GoogleGenerativeAIEmbeddings(model="models/embedding-001", google_api_key=api_key)
                self.provider = "gemini"
        except Exception as e:
            print(f"âš ï¸ BaseProcessor LLM ì´ˆê¸°í™” ì‹¤íŒ¨, í´ë°± ì‚¬ìš©: {e}")
            self.gemini_model = ChatGoogleGenerativeAI(model=gemini_model, google_api_key=api_key)
            self.embeddings_model = GoogleGenerativeAIEmbeddings(model="models/embedding-001", google_api_key=api_key)
            self.provider = "gemini"
        
        # ë°ì´í„° ì €ì¥ì†Œ
        self.templates = []
        self.guidelines = []
        
        # Chroma DB ì»¬ë ‰ì…˜
        self.template_collection = None
        self.guideline_collection = None
        
        # ì¸ë±ìŠ¤ ë§¤ë‹ˆì €
        self.index_manager = get_index_manager()
        
    def encode_texts(self, texts: List[str]) -> np.ndarray:
        """í…ìŠ¤íŠ¸ ë¦¬ìŠ¤íŠ¸ë¥¼ Gemini Embeddingìœ¼ë¡œ ë³€í™˜"""
        try:
            # Gemini Embedding API ì‚¬ìš©
            embeddings = self.embeddings_model.embed_documents(texts)
            return np.array(embeddings)
        except Exception as e:
            print(f" Gemini Embedding ì˜¤ë¥˜: {e}")
            # í´ë°±: ê°„ë‹¨í•œ TF-IDF ê¸°ë°˜ ì„ë² ë”©
            return self._fallback_embedding(texts)
    
    def _fallback_embedding(self, texts: List[str]) -> np.ndarray:
        """í´ë°± ì„ë² ë”© (ê°„ë‹¨í•œ TF-IDF ê¸°ë°˜)"""
        from collections import Counter
        import math
        
        try:
            # ëª¨ë“  ë‹¨ì–´ ìˆ˜ì§‘ ë° ì–´íœ˜ ìƒì„±
            all_words = []
            for text in texts:
                words = text.lower().split()
                all_words.extend(words)
            
            word_counter = Counter(all_words)
            vocab = {word: idx for idx, (word, _) in enumerate(word_counter.most_common(1000))}
            
            # ê°„ë‹¨í•œ TF-IDF ê³„ì‚°
            embeddings = []
            for text in texts:
                words = text.lower().split()
                word_counts = Counter(words)
                
                # TF-IDF ë²¡í„° ìƒì„± (384ì°¨ì›ìœ¼ë¡œ ê³ ì •)
                vector = np.zeros(384)
                for word, count in word_counts.items():
                    if word in vocab and vocab[word] < 384:
                        tf = count / len(words)
                        idf = math.log(len(texts) / (1 + sum(1 for t in texts if word in t.lower().split())))
                        vector[vocab[word]] = tf * idf
                
                embeddings.append(vector)
            
            return np.array(embeddings)
            
        except Exception as e:
            print(f"í´ë°± ì„ë² ë”© ì‹¤íŒ¨: {e}")
            return np.random.rand(len(texts), 384)
    
    def search_similar_chroma(self, query: str, collection_name: str, top_k: int = 3) -> List[Tuple[str, float]]:
        """Chroma DBë¥¼ í†µí•œ ìœ ì‚¬ë„ ê²€ìƒ‰"""
        return self.index_manager.query_similar(
            collection_name=collection_name,
            query_text=query,
            encode_func=self.encode_texts,
            top_k=top_k
        )
    
    def search_similar(self, query: str, collection_name: str, top_k: int = 3) -> List[Tuple[str, float]]:
        """ìœ ì‚¬ë„ ê²€ìƒ‰ (Chroma DB ì‚¬ìš©)"""
        return self.search_similar_chroma(query, collection_name, top_k)
    
    def extract_variables(self, template: str) -> List[str]:
        """í…œí”Œë¦¿ì—ì„œ #{ë³€ìˆ˜ëª…} ë˜ëŠ” #{{ë³€ìˆ˜ëª…}} í˜•íƒœì˜ ë³€ìˆ˜ ì¶”ì¶œ"""
        # ì´ì¤‘ ë¸Œë ˆì´ìŠ¤ì™€ ë‹¨ì¼ ë¸Œë ˆì´ìŠ¤ ëª¨ë‘ ì§€ì›
        pattern = r'#\{+([^}]+)\}+'
        variables = re.findall(pattern, template)
        return list(set(variables))  # ì¤‘ë³µ ì œê±°
    
    def generate_with_gemini(self, prompt: str, max_retries: int = 3) -> str:
        """Geminië¡œ í…ìŠ¤íŠ¸ ìƒì„±"""
        for attempt in range(max_retries):
            try:
                response = self.gemini_model.invoke(prompt)
                return response.content.strip()
            except Exception as e:
                if attempt == max_retries - 1:
                    raise e
                continue
    
    def parse_json_response(self, response_text: str) -> Dict:
        """JSON ì‘ë‹µ íŒŒì‹±"""
        try:
            # JSON ì½”ë“œ ë¸”ë¡ ì œê±°
            if '```json' in response_text:
                json_start = response_text.find('{')
                json_end = response_text.rfind('}') + 1
                json_text = response_text[json_start:json_end]
            else:
                json_text = response_text
            
            return json.loads(json_text)
        except json.JSONDecodeError as e:
            print(f"JSON íŒŒì‹± ì˜¤ë¥˜: {e}")
            return {}
    
    def load_text_file(self, file_path: Path, encoding: str = 'utf-8') -> str:
        """í…ìŠ¤íŠ¸ íŒŒì¼ ë¡œë“œ"""
        try:
            with open(file_path, 'r', encoding=encoding) as f:
                return f.read()
        except Exception as e:
            print(f"íŒŒì¼ ë¡œë“œ ì˜¤ë¥˜ {file_path}: {e}")
            return ""
    
    def chunk_text(self, text: str, chunk_size: int = 800, chunk_overlap: int = 100) -> List[str]:
        """í…ìŠ¤íŠ¸ë¥¼ ì²­í¬ë¡œ ë¶„í• """
        paragraphs = text.split('\n\n')
        chunks = []
        current_chunk = ""
        
        for paragraph in paragraphs:
            if len(current_chunk + paragraph) < chunk_size:
                current_chunk += paragraph + "\n\n"
            else:
                if current_chunk.strip():
                    chunks.append(current_chunk.strip())
                current_chunk = paragraph + "\n\n"
        
        if current_chunk.strip():
            chunks.append(current_chunk.strip())
        
        return [chunk for chunk in chunks if len(chunk.strip()) > 50]