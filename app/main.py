"""
JOBER AI - Main Application Entry Point
FastAPI ê¸°ë°˜ ì•Œë¦¼í†¡ í…œí”Œë¦¿ ìƒì„± ì„œë¹„ìŠ¤
"""

import logging
import os
import sys
from pathlib import Path

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¥¼ Python ê²½ë¡œì— ì¶”ê°€
project_root = Path(__file__).parent.parent
if str(project_root) not in sys.path:
    sys.path.insert(0, str(project_root))

from fastapi import FastAPI, HTTPException, Request
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import JSONResponse
from pydantic import BaseModel
from typing import Dict, Any
from contextlib import asynccontextmanager
import uvicorn

from app.api import templates, health, sessions
from app.utils.llm_provider_manager import get_llm_manager
from config.llm_providers import get_llm_manager as get_config_manager

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


@asynccontextmanager
async def lifespan(app: FastAPI):
    """ì• í”Œë¦¬ì¼€ì´ì…˜ ë¼ì´í”„ì‚¬ì´í´ ê´€ë¦¬"""
    # ì‹œì‘ ì‹œ
    logger.info("ğŸš€ JOBER AI ì„œë¹„ìŠ¤ ì‹œì‘")

    # LLM ì œê³µì ì´ˆê¸°í™” í™•ì¸
    try:
        llm_manager = get_llm_manager()
        status = llm_manager.get_status()
        logger.info(f"LLM ì œê³µì ìƒíƒœ: {status}")

        if not status['available_providers']:
            logger.warning("âš ï¸ ì‚¬ìš© ê°€ëŠ¥í•œ LLM ì œê³µìê°€ ì—†ìŠµë‹ˆë‹¤. API í‚¤ë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”.")
        else:
            logger.info(f"âœ… ì‚¬ìš© ê°€ëŠ¥í•œ LLM ì œê³µì: {status['available_providers']}")

    except Exception as e:
        logger.error(f"âŒ LLM ì œê³µì ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")

    yield

    # ì¢…ë£Œ ì‹œ
    logger.info("ğŸ›‘ JOBER AI ì„œë¹„ìŠ¤ ì¢…ë£Œ")


# FastAPI ì•± ìƒì„±
app = FastAPI(
    title="JOBER AI",
    description="AI ê¸°ë°˜ ì•Œë¦¼í†¡ í…œí”Œë¦¿ ìƒì„± ì„œë¹„ìŠ¤",
    version="2.0.0",
    lifespan=lifespan,
    openapi_tags=[
        {
            "name": "Template Generation",
            "description": "AI ê¸°ë°˜ í…œí”Œë¦¿ ìƒì„± ë° ê´€ë¦¬ API"
        },
        {
            "name": "Real-time Chat",
            "description": "ì‹¤ì‹œê°„ ì±„íŒ… ë° ë³€ìˆ˜ ì—…ë°ì´íŠ¸ API"
        },
        {
            "name": "Session Management",
            "description": "ì„¸ì…˜ ê´€ë¦¬ ë° ëª¨ë‹ˆí„°ë§ API"
        },
        {
            "name": "System",
            "description": "ì‹œìŠ¤í…œ ìƒíƒœ ë° í—¬ìŠ¤ì²´í¬ API"
        }
    ]
)

# CORS ì„¤ì •
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],  # ì‹¤ì œ ë°°í¬ ì‹œì—ëŠ” íŠ¹ì • ë„ë©”ì¸ìœ¼ë¡œ ì œí•œ
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# ê¸€ë¡œë²Œ ì˜ˆì™¸ í•¸ë“¤ëŸ¬
@app.exception_handler(Exception)
async def global_exception_handler(request: Request, exc: Exception):
    """ê¸€ë¡œë²Œ ì˜ˆì™¸ ì²˜ë¦¬"""
    logger.error(f"ê¸€ë¡œë²Œ ì˜ˆì™¸ ë°œìƒ: {exc}", exc_info=True)

    return JSONResponse(
        status_code=500,
        content={
            "detail": {
                "error": {
                    "code": "INTERNAL_SERVER_ERROR",
                    "message": "ì„œë²„ ë‚´ë¶€ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.",
                    "details": str(exc) if os.getenv("DEBUG") == "true" else None
                },
                "timestamp": datetime.now().isoformat() + "Z"
            }
        }
    )

# ë¼ìš°í„° ë“±ë¡
app.include_router(
    health.router,
    prefix="/ai"
)

app.include_router(
    templates.router,
    prefix="/ai"
)

app.include_router(
    sessions.router,
    prefix="/ai"
)

class RootResponse(BaseModel):
    """ë£¨íŠ¸ ì—”ë“œí¬ì¸íŠ¸ ì‘ë‹µ ëª¨ë¸"""
    service: str
    version: str
    description: str
    status: str
    docs: str

# ë£¨íŠ¸ ì—”ë“œí¬ì¸íŠ¸
@app.get("/", tags=["System"], response_model=RootResponse)
async def root():
    """ë£¨íŠ¸ ì—”ë“œí¬ì¸íŠ¸"""
    return {
        "service": "JOBER AI",
        "version": "2.0.0",
        "description": "AI ê¸°ë°˜ ì•Œë¦¼í†¡ í…œí”Œë¦¿ ìƒì„± ì„œë¹„ìŠ¤",
        "status": "running",
        "docs": "/docs"
    }

class LLMStatusMainResponse(BaseModel):
    """ë©”ì¸ LLM ìƒíƒœ ì‘ë‹µ ëª¨ë¸"""
    available_providers: list
    primary_provider: str
    failure_counts: Dict[str, int]
    gemini_configured: bool
    openai_configured: bool

# LLM ì œê³µì ìƒíƒœ í™•ì¸ ì—”ë“œí¬ì¸íŠ¸
@app.get("/ai/llm/status", tags=["System"], response_model=LLMStatusMainResponse)
async def llm_status():
    """LLM ì œê³µì ìƒíƒœ í™•ì¸"""
    try:
        manager = get_llm_manager()
        return manager.get_status()
    except Exception as e:
        raise HTTPException(
            status_code=500,
            detail=f"LLM ìƒíƒœ í™•ì¸ ì‹¤íŒ¨: {e}"
        )


if __name__ == "__main__":
    # ê°œë°œ ì„œë²„ ì‹¤í–‰
    port = int(os.getenv("PORT", 8000))
    host = os.getenv("HOST", "0.0.0.0")

    logger.info(f"ê°œë°œ ì„œë²„ ì‹œì‘: http://{host}:{port}")

    uvicorn.run(
        "app.main:app",
        host=host,
        port=port,
        reload=True,
        log_level="info"
    )